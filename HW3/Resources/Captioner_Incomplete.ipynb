{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "tXTOB1LQrToh",
        "gTLoIMsTrZOF"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "1. **Translation Model**:\n",
        "In this homework assignment, the initial focus is on implementing a Translation Model. This component aims to facilitate language translation, using the Seamless model.\n",
        "\n",
        "2. **ClipCap Model**:\n",
        "The second major section involves the creation of a Captioner model utilizing CLIP. This model is designed to generate captions, in response to visual input.\n",
        "\n",
        "3. **ClipCap + Translation**:\n",
        "The final part of the notebook combines the capabilities of the ClipCap model with the Translation Model. This integration suggests a comprehensive solution where CLIP-generated captions are subsequently translated into Farsi using the Seamless model."
      ],
      "metadata": {
        "id": "TGB0s6DlenuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "TG9N24vRgdG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece\n",
        "!pip install open-flamingo\n",
        "!pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "NwqmNq7uJvc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import clip\n",
        "import torch\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from enum import Enum\n",
        "import skimage.io as io\n",
        "import torch.nn.functional as nnf\n",
        "from typing import Tuple, List, Union, Optional\n",
        "from transformers import AutoProcessor, SeamlessM4TModel\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "1gjdPnRdEvdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Drive Downloader"
      ],
      "metadata": {
        "id": "TBPiURzh4E6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't touch this part !!"
      ],
      "metadata": {
        "id": "DVS1goHug2s_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "download_with_pydrive = True\n",
        "\n",
        "class Downloader(object):\n",
        "    def __init__(self, use_pydrive):\n",
        "        self.use_pydrive = use_pydrive\n",
        "\n",
        "        if self.use_pydrive:\n",
        "            self.authenticate()\n",
        "\n",
        "    def authenticate(self):\n",
        "        auth.authenticate_user()\n",
        "        gauth = GoogleAuth()\n",
        "        gauth.credentials = GoogleCredentials.get_application_default()\n",
        "        self.drive = GoogleDrive(gauth)\n",
        "\n",
        "    def download_file(self, file_id, file_dst):\n",
        "        if self.use_pydrive:\n",
        "            downloaded = self.drive.CreateFile({'id':file_id})\n",
        "            downloaded.FetchMetadata(fetch_all=True)\n",
        "            downloaded.GetContentFile(file_dst)\n",
        "        else:\n",
        "            !gdown --id $file_id -O $file_dst\n",
        "\n",
        "downloader = Downloader(download_with_pydrive)"
      ],
      "metadata": {
        "id": "gne590Qo4HD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation"
      ],
      "metadata": {
        "id": "79uYDFsEEIgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this segment, our objective is to employ a pre-existing translation model for converting text between English and Farsi. Your task is to finalize the provided functions and test them on the provided texts."
      ],
      "metadata": {
        "id": "zsVKIa6Zg_N9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ck10ZJgbD_qk"
      },
      "outputs": [],
      "source": [
        "processor = AutoProcessor.from_pretrained(\"facebook/hf-seamless-m4t-medium\")\n",
        "model = SeamlessM4TModel.from_pretrained(\"facebook/hf-seamless-m4t-medium\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_english_to_persian(text):\n",
        "\n",
        "    # ======================================= Code =======================================\n",
        "\n",
        "    # ======================================= Code =======================================\n",
        "\n",
        "    return translated_text_from_text"
      ],
      "metadata": {
        "id": "jTHaRYvaI5cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_persian_to_english(text):\n",
        "\n",
        "    # ======================================= Code =======================================\n",
        "\n",
        "    # ======================================= Code =======================================\n",
        "\n",
        "    return translated_text_from_text"
      ],
      "metadata": {
        "id": "h_aZY40YKi6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_english_to_persian('Levi is the best anime character ever!')"
      ],
      "metadata": {
        "id": "it4wsSylMBRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_persian_to_english('انسان به منافع هیچ موجودی جز خود نمی‌اندیشد')"
      ],
      "metadata": {
        "id": "vfrkCGjfMHKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ClipCap"
      ],
      "metadata": {
        "id": "yR5WSQNJrOJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, our goal is to complete the implementation of the ClipCap model. To achieve this, it's crucial to thoroughly review the associated paper to understant the model's functioning. Please ensure that modifications are solely made within the specified section, avoiding alterations elsewhere in the model to prevent potential issues when loading weights."
      ],
      "metadata": {
        "id": "q-5-qcSTprAq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Base Modules"
      ],
      "metadata": {
        "id": "tXTOB1LQrToh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't touch this either :)"
      ],
      "metadata": {
        "id": "Zi1ti1r2pjCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MappingType(Enum):\n",
        "    MLP = 'mlp'\n",
        "    Transformer = 'transformer'\n",
        "\n",
        "\n",
        "class MlpTransformer(nn.Module):\n",
        "     def __init__(self, in_dim, h_dim, out_d: Optional[int] = None, act=nnf.relu, dropout=0.):\n",
        "         super().__init__()\n",
        "         out_d = out_d if out_d is not None else in_dim\n",
        "         self.fc1 = nn.Linear(in_dim, h_dim)\n",
        "         self.act = act\n",
        "         self.fc2 = nn.Linear(h_dim, out_d)\n",
        "         self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "     def forward(self, x):\n",
        "         x = self.fc1(x)\n",
        "         x = self.act(x)\n",
        "         x = self.dropout(x)\n",
        "         x = self.fc2(x)\n",
        "         x = self.dropout(x)\n",
        "         return x\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n",
        "        super(MLP, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(sizes) - 1):\n",
        "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n",
        "            if i < len(sizes) - 2:\n",
        "                layers.append(act())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, dim_self, dim_ref, num_heads, bias=True, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim_self // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "        self.to_queries = nn.Linear(dim_self, dim_self, bias=bias)\n",
        "        self.to_keys_values = nn.Linear(dim_ref, dim_self * 2, bias=bias)\n",
        "        self.project = nn.Linear(dim_self, dim_self)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        y = y if y is not None else x\n",
        "        b, n, c = x.shape\n",
        "        _, m, d = y.shape\n",
        "        # b n h dh\n",
        "        queries = self.to_queries(x).reshape(b, n, self.num_heads, c // self.num_heads)\n",
        "        # b m 2 h dh\n",
        "        keys_values = self.to_keys_values(y).reshape(b, m, 2, self.num_heads, c // self.num_heads)\n",
        "        keys, values = keys_values[:, :, 0], keys_values[:, :, 1]\n",
        "        attention = torch.einsum('bnhd,bmhd->bnmh', queries, keys) * self.scale\n",
        "        if mask is not None:\n",
        "            if mask.dim() == 2:\n",
        "                mask = mask.unsqueeze(1)\n",
        "            attention = attention.masked_fill(mask.unsqueeze(3), float(\"-inf\"))\n",
        "        attention = attention.softmax(dim=2)\n",
        "        out = torch.einsum('bnmh,bmhd->bnhd', attention, values).reshape(b, n, c)\n",
        "        out = self.project(out)\n",
        "        return out, attention\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "\n",
        "    def forward_with_attention(self, x, y=None, mask=None):\n",
        "        x_, attention = self.attn(self.norm1(x), y, mask)\n",
        "        x = x + x_\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x, attention\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        x = x + self.attn(self.norm1(x), y, mask)[0]\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "    def __init__(self, dim_self, dim_ref, num_heads, mlp_ratio=4., bias=False, dropout=0., act=nnf.relu,\n",
        "                 norm_layer: nn.Module = nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim_self)\n",
        "        self.attn = MultiHeadAttention(dim_self, dim_ref, num_heads, bias=bias, dropout=dropout)\n",
        "        self.norm2 = norm_layer(dim_self)\n",
        "        self.mlp = MlpTransformer(dim_self, int(dim_self * mlp_ratio), act=act, dropout=dropout)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "\n",
        "    def forward_with_attention(self, x, y=None, mask=None):\n",
        "        attentions = []\n",
        "        for layer in self.layers:\n",
        "            x, att = layer.forward_with_attention(x, y, mask)\n",
        "            attentions.append(att)\n",
        "        return x, attentions\n",
        "\n",
        "    def forward(self, x, y=None, mask=None):\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            if i % 2 == 0 and self.enc_dec: # cross\n",
        "                x = layer(x, y)\n",
        "            elif self.enc_dec:  # self\n",
        "                x = layer(x, x, mask)\n",
        "            else:  # self or cross\n",
        "                x = layer(x, y, mask)\n",
        "        return x\n",
        "\n",
        "    def __init__(self, dim_self: int, num_heads: int, num_layers: int, dim_ref: Optional[int] = None,\n",
        "                 mlp_ratio: float = 2., act=nnf.relu, norm_layer: nn.Module = nn.LayerNorm, enc_dec: bool = False):\n",
        "        super(Transformer, self).__init__()\n",
        "        dim_ref = dim_ref if dim_ref is not None else dim_self\n",
        "        self.enc_dec = enc_dec\n",
        "        if enc_dec:\n",
        "            num_layers = num_layers * 2\n",
        "        layers = []\n",
        "        for i in range(num_layers):\n",
        "            if i % 2 == 0 and enc_dec:  # cross\n",
        "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "            elif enc_dec:  # self\n",
        "                layers.append(TransformerLayer(dim_self, dim_self, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "            else:  # self or cross\n",
        "                layers.append(TransformerLayer(dim_self, dim_ref, num_heads, mlp_ratio, act=act, norm_layer=norm_layer))\n",
        "        self.layers = nn.ModuleList(layers)\n",
        "\n",
        "\n",
        "class TransformerMapper(nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear(x).view(x.shape[0], self.clip_length, -1)\n",
        "        prefix = self.prefix_const.unsqueeze(0).expand(x.shape[0], *self.prefix_const.shape)\n",
        "        prefix = torch.cat((x, prefix), dim=1)\n",
        "        out = self.transformer(prefix)[:, self.clip_length:]\n",
        "        return out\n",
        "\n",
        "    def __init__(self, dim_clip: int, dim_embedding: int, prefix_length: int, clip_length: int, num_layers: int = 8):\n",
        "        super(TransformerMapper, self).__init__()\n",
        "        self.clip_length = clip_length\n",
        "        self.transformer = Transformer(dim_embedding, 8, num_layers)\n",
        "        self.linear = nn.Linear(dim_clip, clip_length * dim_embedding)\n",
        "        self.prefix_const = nn.Parameter(torch.randn(prefix_length, dim_embedding), requires_grad=True)"
      ],
      "metadata": {
        "id": "zprPi4gYrMJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Modules"
      ],
      "metadata": {
        "id": "gTLoIMsTrZOF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ClipCaptionModel(nn.Module):\n",
        "\n",
        "    def get_dummy_token(self, batch_size: int, device: torch.device) -> torch.Tensor:\n",
        "        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n",
        "\n",
        "    def __init__(self, prefix_length: int, clip_length: Optional[int] = None, prefix_size: int = 512,\n",
        "                 num_layers: int = 8, mapping_type: MappingType = MappingType.MLP):\n",
        "        super(ClipCaptionModel, self).__init__()\n",
        "        self.prefix_length = prefix_length\n",
        "        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
        "        if mapping_type == MappingType.MLP:\n",
        "            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2,\n",
        "                                     self.gpt_embedding_size * prefix_length))\n",
        "        else:\n",
        "            self.clip_project = TransformerMapper(prefix_size, self.gpt_embedding_size, prefix_length,\n",
        "                                                                     clip_length, num_layers)\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, prefix: torch.Tensor, mask: Optional[torch.Tensor] = None,\n",
        "                labels: Optional[torch.Tensor] = None):\n",
        "\n",
        "        # ==================================== Code ====================================\n",
        "\n",
        "        # ==================================== Code ====================================\n",
        "\n",
        "        return out\n",
        "\n",
        "class ClipCaptionPrefix(ClipCaptionModel):\n",
        "\n",
        "    def parameters(self, recurse: bool = True):\n",
        "        return self.clip_project.parameters()\n",
        "\n",
        "    def train(self, mode: bool = True):\n",
        "        super(ClipCaptionPrefix, self).train(mode)\n",
        "        self.gpt.eval()\n",
        "        return self"
      ],
      "metadata": {
        "id": "wC79AV1xrbER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Caption Generation Functions"
      ],
      "metadata": {
        "id": "6Duui4591YJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, you are to implement two decoding strategies, specifically, both greedy and beam search. Your task involves completing the provided functions dedicated to these strategies."
      ],
      "metadata": {
        "id": "DiT7zqYJq3IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n",
        "                  entry_length=67, temperature=1., stop_token: str = '.'):\n",
        "\n",
        "    # ==================================== Code (Begin) ====================================\n",
        "\n",
        "    # ==================================== Code (End) ====================================\n",
        "\n",
        "def generate_simple(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        tokens=None,\n",
        "        prompt=None,\n",
        "        embed=None,\n",
        "        entry_count=1,\n",
        "        entry_length=67,  # maximum number of words\n",
        "        top_p=0.8,\n",
        "        temperature=1.,\n",
        "        stop_token: str = '.',\n",
        "):\n",
        "\n",
        "    # ==================================== Code (Begin) ====================================\n",
        "\n",
        "    # ==================================== Code (End) ===================================="
      ],
      "metadata": {
        "id": "HwAtiZQW1XIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate"
      ],
      "metadata": {
        "id": "k6kUPXe-3cnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model = 'COCO'\n",
        "model_path = './model_wieghts.pt'\n",
        "current_directory = os.getcwd()\n",
        "\n",
        "downloader.download_file(\"1GYPToCqFREwi285wPLhuVExlz7DDUDfJ\", model_path)\n",
        "is_gpu = False\n",
        "device = 'cuda:0' if is_gpu else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"RN50x4\", device=device, jit=False)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
      ],
      "metadata": {
        "id": "RJ-z8e0V3hFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prefix_length = 40\n",
        "model = ClipCaptionPrefix(prefix_length, clip_length=40, prefix_size=640, num_layers=8, mapping_type='transformer')\n",
        "model.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
        "model = model.eval()\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "QC-a_MCL7WG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ['562207', '579664', '060623', '165547', '334321', '483108', '386164', '354533']\n",
        "IMAGE_NAME = '334321'\n",
        "\n",
        "name_ = \"COCO_val2014_000000\" + IMAGE_NAME + \".jpg\"\n",
        "images_path = os.path.join(os.path.dirname(current_directory), \"images\")\n",
        "os.makedirs(images_path, exist_ok=True)\n",
        "UPLOADED_FILE = os.path.join(images_path, name_)\n",
        "\n",
        "if not os.path.isfile(UPLOADED_FILE):\n",
        "  download_path = os.path.join(images_path, \"images.zip\")\n",
        "  downloader.download_file(\"1l6J9WFYxpF-1HFr3A5Oq1eoObTxzbPgs\", download_path)\n",
        "  !unzip {download_path} -d {images_path}"
      ],
      "metadata": {
        "id": "7sOtrLbbHc6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the next cell use the ClipCap model to generate captions for the image."
      ],
      "metadata": {
        "id": "YnbnSsL5sJgV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_beam_search = True\n",
        "image = io.imread(UPLOADED_FILE)\n",
        "pil_image = PIL.Image.fromarray(image)\n",
        "display(pil_image)\n",
        "\n",
        "# ==================================== Code (Begin) ====================================\n",
        "\n",
        "# ==================================== Code (End) ===================================="
      ],
      "metadata": {
        "id": "MLsffURbIp6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ClipCap + Seamless"
      ],
      "metadata": {
        "id": "eMeaynBQbHKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, your task is to employ the Seamless model to translate the generated captions into Farsi."
      ],
      "metadata": {
        "id": "UK6ChUvAsjSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================== Code (Begin) ====================================\n",
        "\n",
        "# ==================================== Code (End) ===================================="
      ],
      "metadata": {
        "id": "undX9zoccBke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "lcr3QU3rshKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to evaluate your model, first you need to prepare a dataset with 100 pairs of image and Farsi captions. For this you can use the images and captions available in the COCO dataset (you need to translate the captions to Farsi). Then use your model to predict captions for the images and finally use common metrics such as BLEU to evaluate your model."
      ],
      "metadata": {
        "id": "Xt79gu8EslWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================================== Code (Begin) ====================================\n",
        "\n",
        "# ==================================== Code (End) ===================================="
      ],
      "metadata": {
        "id": "o_m_VSRysiLk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}